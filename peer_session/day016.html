<h1 id="-4-02-15-">피어세션기록(4주차, 02.15 월)</h1>
<h2 id="-">오늘 수업/실습/과제</h2>
<ul>
<li><p>질문</p>
<ul>
<li><p>(성익) Word2Vec에서 히든 레이어의 수는 정해져있는건가요? 좌표공간 차원수는 원-핫벡터 크기 아닌가요?</p>
<ul>
<li>(준호) hidden layer의 노드 수는 하이퍼파라미터라 관계없다. 그냥 feature의 차원 수</li>
<li>(지원) 좌표 공간 차원수는 feature 수이고, 이건 원-핫벡터 사이즈랑 관계없음.</li>
</ul>
</li>
<li><p>(성익) class와 label 차이?</p>
<ul>
<li>(준배) 클래스는 일종의 카테고리, label은 데이터에 대한 값</li>
<li>(준호) 레이블링은 지도학습을 위한 ground truth를 지정해주는것이니까.. 데이터에 대한 classification 값이 label.</li>
</ul>
</li>
<li><p>(성익) Word2Vec에서 Input vector와 output vector가 같다는게 무슨말인가요?</p>
<ul>
<li>(지원) Word embedding은 거리를 기준으로 관계도만 측정하는거기 때문에, Input vector만 사용하든 Output vector만 사용하든 상관없다.</li>
</ul>
</li>
<li><p>(현우) 원-핫벡터에서 dimension이 뭔가요? 그냥 1차원 벡터아닌가요?</p>
<ul>
<li>(성익) 원-핫벡터는 컴퓨터가 알아듣도록 만드는 1차원 array일 뿐이고, 실제 의미는 공간좌표에서 해당 단어(점)의 위치.</li>
<li>(준호) hidden layer에서 feature dimension과 같은 맥락.</li>
<li>(지원) tensor에서의 괄호랑은 다른거죠?<ul>
<li>(성익) 저는 비슷하게 생각했는데, tensor도 결국 공간좌표상의 여러 점들을 모아놓은것 아닌가요?</li>
<li>(준호) 원-핫 벡터의 차원이랑은 다른 느낌이긴 한거 같아요.</li>
</ul>
</li>
</ul>
</li>
<li><p>(성익) Loss Function, Cost Function, Objective Function 차이?</p>
<ul>
<li>Loss는 하나의 데이터 포인트에 대해, Cost는 전체 데이터셋에 대한 Cost.</li>
<li>Objective는 머신러닝 이외에도 사용하는, 목적을 이루기 위한 함수식. 딥러닝에서는 Cost Function과 비슷하게 사용되는듯</li>
<li>(머신러닝 - Loss Function, Cost Function, Objective Function의 차이)[<a href="http://blog.naver.com/PostView.nhn?blogId=qbxlvnf11&amp;logNo=221386278997&amp;parentCategoryNo=&amp;categoryNo=&amp;viewDate=&amp;isShowPopularPosts=false&amp;from=postView">http://blog.naver.com/PostView.nhn?blogId=qbxlvnf11&amp;logNo=221386278997&amp;parentCategoryNo=&amp;categoryNo=&amp;viewDate=&amp;isShowPopularPosts=false&amp;from=postView</a>]</li>
</ul>
</li>
<li><p>(성익) 역전파 과정 설명?</p>
<ul>
<li>(준호) <a href="http://blog.naver.com/PostView.nhn?blogId=qbxlvnf11&amp;logNo=221386278997&amp;parentCategoryNo=&amp;categoryNo=&amp;viewDate=&amp;isShowPopularPosts=false&amp;from=postView">cs-231n</a></li>
<li>(지원) <a href="https://cs231n.github.io/optimization-2/">cs-231n 강의노트</a></li>
</ul>
</li>
</ul>
</li>
<li><p>이번주는 실습 위주라서, 실습을 좀 빡세게 잡는게 좋을듯.</p>
</li>
<li><p>(futher question)Word2Vec과 GloVe 알고리즘이 가지고 있는 단점은 무엇일까요?</p>
<ul>
<li>(성익) 단락의 양이 많아지거나 하면, 멀리 있는 것과의 연관성이 없을것같다.</li>
<li>(준호) 순서를 파악하지 못할 것 같다.</li>
<li>GloVe의 문제점<ul>
<li>(현우) 윈도우를 크게 잡아서 한번에 다 count해야 해서 리소스가 많이 든다.</li>
</ul>
</li>
<li>(준호) Word2Vec은 주어진 윈도우 안에서 학습 예측을 다 하는거고.</li>
<li>(준호) GloVe 윈도우 사이즈는 중요하지 않고, 사전 통계를 만들어 놓고 새로운 단어를 추측하는것.</li>
<li><a href="https://lovit.github.io/nlp/representation/2018/09/05/glove/">GloVe 구현 코드</a></li>
</ul>
</li>
</ul>
<h2 id="-">정보</h2>
<ul>
<li>(준호) 파이토치 단어임베딩 튜토리얼(<a href="https://tutorials.pytorch.kr/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py">https://tutorials.pytorch.kr/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py</a>)</li>
</ul>
