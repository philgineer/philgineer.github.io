---
title: "Boostcamp AI Tech (Day 009)"
date: 2021-01-28
layout: post
tags: [Naver Boostcamp, daily report]
use_math: true
---

## 1. Pandas (2)

* Groupby
    * SQL groupby 명령어와 동일
    * split - apply - combine 과정을 거쳐 연산
        ```python
        # groupby(묶음의 기준 컬럼)[적용받는 컬럼].적용받는 연산
        df.groupby("Team")["Points"].sum()
        ```
* apply 유형
    * aggregation: 요약된 통계정보 추출 ex. sum, mean
    * transformation: 해당 정보 변환 ex. lambda
    * filtration: 특정 정보 제거 (필터링)
        ```python
        grouped.agg(sum)
        grouped.agg(np.mean)

        # normalize
        score = lambda x: (x - x.mean()) / x.std()
        grouped.transform(socore)

        df.groupby("Team").filter(lambda x: x["Points"].max() > 700)
        ```
* pivot table
    * index 축은 groupby와 동일
    * column에 추가로 labeling 값 추가
    * value에 numeric type 값을 aggregation
        ```python
        # 값: duration
        df.pivot_table([duration],
                        index=[df.month, df.item],
                        columns=df.network,
                        aggfunc="sum",
                        fill_value=0)
        ```
* merge & concat
    ```python
    pd.merge(df_a, df_b, on='subject_id')
    # pd.merge(df_a, df_b, left_on='subject_id', right_on='id', how='right')
    # join: inner, left, right, full
    pd.concat([df_a, df_b])
    ```
* persistence
    * db 연결 conn을 사용해 dataframe 생성
        ```python
        import sqlite3

        conn = sqlite3.connet('./data.db')
        cur = conn.cursor()
        cur.execute('select * from airlines limit 5;')
        res = cur.fetchall()
        # tuple 형태로 나옴
        df_airlines = pd.read_sql_query('select * from airlines;', conn)
        ```
    * XLS
        * openpyxls, XlsxWrite
            ```python
            writer = pd.ExcelWriter('./df_routes.xlsx', engine='xlswriter')
            df_routes.to_excel(writer, sheet_name='Sheet1')
            df_routes.to_pickle('./df_routes.pickle')
            ```
## 2. 확률론

* 딥러닝과 확률론
    * 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있음
    * loss functions의 작동 원리는 데이터 공간을 통계적으로 해석해서 유도함
    * 회귀 분석에서 손실함수로 사용되는 L2-norm은 예측오차의 분산을 최소화하는 방향으로 학습하도록 유도
    * 분류문제에서 사용되는 cross-entropy는 모델 예측의 불확실성을 최소화하는 방향으로 학습하도록 유도
    * 분산과 불확실성을 최소화하기 위해서는 측정 방법을 알아야 함
* 확률분포는 데이터의 초상화
    * 데이터공간: $\mathcal X \times \mathcal Y$
    * 데이터를 추출하는 분포: $\mathcal D$
    * 데이터 확률변수: $(X, y) \sim \mathcal D$
* 확률변수
    * 확률 분포 $\mathcal D$에 따라 이산형/연속형으로 구분됨
    * discrete: 확률변수가 가질 수 있는 경우의 수를 모두 고려해 확률을 더해서 모델링

        $\mathbb P(X \in A) = \sum_{X \in A}P(X = x)$

    * continuous: 데이터 공간에 정의된 확률변수의 밀도(density) 위에서의 적분을 통해 모델링

        $\mathbb P(X \in A) = \int_A P(x)dx$
    
    * 밀도는 누적확률분포의 변화율을 모델링
* 확률분포
    * 결합분포(joint distribution) $P(X, y)$는 $\mathcal D$를 모델링
    * $\mathcal D$는 이론적으로 존재하는 확률분포기 때문에 사전에 알 수 없음
    * $P(X)$는 입력 $X$에 대한 주변확률분포로 $y$에 대한 정보를 주진 않음
    * $P(X) = \sum_y P(X, y)$
    * $P(X) = \int_{\mathcal y} P(X, y)dy$
    * 조건부확률분포 $P(X|y)$는 데이터 공간에서 입력 X와 출력 y 사이의 관계를 모델링 (특정 클래스가 주어진 조건에서 데이터의 확률분포)
* 조건부확률과 기계학습
    * $P(X|y)$: 입력변수 X에 대해 정답이 y일 확률
    * 로지스틱 회귀에서 사용했던 선형모델과 소프트맥스 함수의 결합은, 데이터에서 추출된 패턴을 기반으로 확률을 해석하는 데 사용
    * 분류 문제에서 $softmax(W\phi + b)$는 데이터 X로부터 추출된 특징패턴 $\phi(x)$와 가중치행렬 $W$를 통해 조건부확률 $P(X|y)$를 계산함
    * 회귀 문제의 경우 조건부기대값 $\mathbb E[y|X]$를 추정함
    * $\mathbb E_{y \sim P(y|X)}[y|X] = \int_{\mathcal y} yP(y|X)dy)$
    * 조건부기대값은 $\mathbb E \Vert y - f(x) \Vert_2$를 최소화하는 함수 $f(x)$와 일치함
    * 딥러닝은 다층신경망을 사용해 데이터로부터 특징패턴 $\phi$를 추출
* 기대값(expectation)
    * 확률분포가 주어지면 데이터를 분석하는 데 사용 가능한 여러 종류의 통계적 범함수(statistical function)를 계산할 수 있음
    * 기대값은 데이터를 대표하는 통계량이면서, 동시에 확률분포를 통해 다른 통계적 범함수를 계산하는 데 사용됨

        이산확률분포: $\mathbb E_{X \sim P(X)}[f(X)] = \sum_{X \in \mathcal X} f(X)P(X)$

        연속확률분포: $\mathbb E_{X \sim P(X)}[f(X)] = \int_{\mathcal X} f(X)P(X)dX$
    * 기대값을 이용해 분산, 첨도, 공분산 등 계산 가능

        $\mathbb V (X) = \mathbb E_{X \sim P(X)}[X - \mathbb E [X]^2]$

        $Cov(X_1, X_2) = \mathbb E_{X_1,X_2 \sim P(X_1, X_2)} [(X_1 - \mathbb E [X_1])] [(X_2 - \mathbb E [X_2])]$
* 몬테카를로(Monte Carlo) 샘플링
    * 기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때가 많음
    * 확률분포를 모를 때 데이터를 이용해 기대값을 계산할 때 몬테카를로 샘플링 방법을 사용
    * 몬테카를로는 이산형/연속형 상관없이 성립 (i.i.d. $X^{(i)} \sim P(X)$)

        $\mathbb E_{X \sim P(X)} [f(X)] \approx \frac{1}{N} \sum_{i=1}^N f(X^{(i)})$

    * 몬테카를로 샘플링은 독립추출만 보장된다면, 대수의 법칙(law of large number)에 의해 수렴성을 보장함
