---
title: "Boostcamp AI Tech (P4 - Day05)"
date: 2021-05-28
layout: post
tags: [Naver Boostcamp, daily report]
use_math: true
---
## 프로젝트 진행
* LB 0.6510 달성
    * Model
        * encoder: CNN
        * decoder: SATRN
        * epoch: 37
        * batch_size: 36
        * forcing-ratio: 0.5
        * drop-out: 0.1
        * optimizer: AdamW
    * Sentence Acc
        * LB: 0.6240
        * Val: 0.6307
    * LB Word Error Rate
        * LB: 0.1061
        * Val: 0.0978
    * AdamW가 일정 수준 일반화에 도움된 듯

## 한 주 team 회고

### 좋았던일
- GRU 디버깅
- RGB를 Black & White 이미지로 바꿔서 좀 더 또렷하게 데이터를 만들어봄
- 기본 베이스 코드 실행 및 제출을 했다

### 아쉬웠던부분
- 이번주는 방향성을 잘 못고 혼자 어리둥절했습니다.
- 다른 모델을 시도해보지 못한것
- 베이스코드 실행하는데 공지도 늦게나오다보니 느리다고 느꼈습니다

### 다음 도전할것
- SRN 포팅하기 
- 데이터쪽에서 인사이트 찾기
    - 외부 데이터 추가
    - 데이터 노이즈 최소화
    - filp된거 어떻게 할거?
    - 다른 OCR 논문에서 aug를 한건가?안한건가?
    - mixup 한번 넣어볼까
- encoder변경, transformer 적용
    - encoder를 efficientnet 같은거 써볼까 아니면 resnet도?
- wandb 연동하기
- 페이퍼로 검증된 기법을 도전

### 이번주 느낀점
- 아직 방향성을 잡지 못하고 있습니다.

### 한일
- attention베이스 50+50
    - 조금 향상했다
- SATRN teacher forcing 1.0
    - 성능이 별로 않좋음
    - dropout이 문제였던 걸까?
- GRU
    - 아직까지는 (22 epoch 까지는 train acc와 val acc가 같이 우상향이라 좋다)
    - 솔직히 Attention.py 마스터했다
- 동일 task paper 정리
    - [Augmentation에 도움될 만한 paper](https://arxiv.org/pdf/1901.06763.pdf)
    - [Dense encoder + multi-scale attention 기법 paper](https://arxiv.org/pdf/1801.03530.pdf)
    - [아키텍처 및 하이퍼파라미터 참고할 만한 paper](http://cs231n.stanford.edu/reports/2017/pdfs/815.pdf)
- Dataset 분석
    - 숫자,문자는 예측을 잘하지만 아무래도 괄호 {}예측을 조금더 늦게 하는것같음.
        - {} 이 부분을 얼마나 잘 예측하는가가 key point가 되지 않을까??
    - 일반 문자예측은 SATRN기준으로 20epoch만 가도 거의 예측을 하는것을 볼 수 있음.
- Optimizer 수정하여 실험하였으나, Adadelta 사용시 에러가 발생함
    - adam부분에 adadelta로 그냥 치환해서 실험안한듯..
-  SATRN (default)
    - [텐서플로 링크](  http://118.67.132.36:6008/#scalars&runSelectionState=eyIuIjp0cnVlfQ%3D%3D&tagFilter=loss)
    - validation score로 봤을 때는 attention에 비해 0.1 이상 높음
    - AdamW로 optimizer 실험도 동시에 학습 중
- Attention 코드를 2시간 동안 분석하여 GRU 포팅에 성공
    - 👍+6 (히히헤헤)
- baseline에서 구현이 안 되어 있는 부분들 개선 작업
    - adadelta optimizer 사용 시 scheduler (beta 값) 오류

### 생각
- attention은 가벼운 모델로 사용이 나을듯
- 데이터 증강
    - 10만장 뻥튀기는 많나?
- Augmentation: 이미지를 90, 180, 360을 돌려서 테스트하면 성능이 좋아지지 않을까?
    - 수식이 90도나 180로 되어있는 이미지가 존재
    - random flip
    - 기홍님 Base SATRN에 실험을 해서 정확한 비교를 하게 하자
- 대체 grad norm은 뭐지???
    - ARABOZA

### 꿀팁
- screen 사용법
    - 장점: 학습을 시키면서 컴퓨터를 끌 수 있다.
    - 설치apt-get install screen
    - 터미널 실행 명령어 : screen
    - 터미널 detach: cltr + A +D 키
    - 재실행: screen -r
    - https://www.notion.so/Screen-0299ba5a72de4c01b3a8291140040033 
