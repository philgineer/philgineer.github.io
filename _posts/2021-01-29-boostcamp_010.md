---
title: "Boostcamp AI Tech (Day 010)"
date: 2021-01-29
layout: post
tags: [Naver Boostcamp, daily report]
use_math: true
---

## 1. 시각화

* ### matplotlib
    * pyplot 객체에 그래프들을 쌓은 다음 flush
    * figure 안에 여러 개의 axes로 구성
        ```python
        fig = plt.figure()
        fig.set_size_inches(10, 6)  # size 설정
        plt.style.use("ggplot")    # 스타일 적용

        ax = []
        colors = ["b", "g", "r", "c", "m", "y", "k"]
        for i in range(1, 7):
            ax.append(fig.add_subplot(2, 3, i))  # 두 개, 세 줄의 plot 생성
            X_1 = np.arange(50)
            Y_1 = np.random.rand(50)
            c = colors[np.random.randint(1, len(colors))]

            ax[i - 1].plot(X_1, Y_1, c=c)
        ```
* ### plt 속성
    * [matplotlib.org/tutorials](https://matplotlib.org/tutorials/index.html)
    * plot, scatter, plt.bar, hist, boxplot
<br><br>

## 2. 통계학

* ### 통계적 모델링
    * 적절한 가정 위에서 확률분포를 추정(inference)하는 것이 목표
    * 기계학습과 통계학이 공통적으로 추구하는 목표
    * 유한한 개수의 데이터만 관찰해서 모집단의 분포를 정확하게 알 수 없기 때문에, 근사적으로 확률분포를 추정
    * 예측모형의 목적은 데이터와 추정 방법의 불확실성을 고려해 위험을 최소화하는 것
    * **모수적(parametric) 방법론**: 데이터가 특정 확률분포를 따른다고 선험적으로(a priori) 가정한 후, 그 분포를 결정하는 모수를 추정하는 방법
    * **비모수(nonparametric) 방법론**: 특정 확률분포를 가정하지 않고, (모수가 무한히 많거나) 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀜 (ML 대부분의 방법론)
* ### 확률분포 가정하기
    * 우선 히스토그램을 통해 모양을 관찰
    * **데이터를 생성하는 원리**를 먼저 고려하는 것이 원칙
    * 데이터가 2개의 값을 가짐 -> 베르누이분포
    * 데이터가 n개의 이산적인 값을 가짐 -> 카테고리(멀티누이)분포
    * 데이터가 $[0,1]$ 사이의 값 가짐 -> 베타분포
    * 데이터가 0 이상의 값 가짐 -> 감마분포, 로그정규분포 등
    * 데이터가 $\mathbb R$ 전체에서 값 가짐 -> 정규분포, 라플라스분포 등
* ### 데이터로 모수 추정하기
    * 데이터의 확률분포를 가정했다면 모수 추정 가능
    * 정규분포의 모수는 평균 $\mu$과 분산 $\sigma^2$으로 이를 추정하는 통계량(statistic)은 다음과 같음

        ||표본 통계량|(추정된) 모수|
        |---|---|---|---|
        |평균|$\bar{X} = \frac{1}{N} \sum_{i=1}^N X_i$|$\mathbb E [\bar{X}] = \mu$|
        |분산|$S^2 = \frac{1}{N - 1} \sum_{i=1}^N (X_i - \bar{X})^2$|$\mathbb E [S^2] = \sigma^2$|

    * 표본 분산을 구할 때 N - 1로 나누는 이유는 불편(unbiased) 추정량을 구하기 위함임

* ### 표본분포와 표집분포
    * 표본분포(sample dist.): 단순 표본들의 분포를 뜻하며, N이 커져도 정규분포를 따르지 않음
    * 표집분포(sampling dist.): 통계량(표본평균과 표본분산)의 확률 분포.
    * 표집분포의 경우 N이 커질수록 정규분포 $\mathcal N (\mu, \sigma^2 / N)$을 따름 $\Rightarrow$ **중심극한정리** (Central Limit Theorem)
    * 중심극한정리는 모집단의 분포가 정규분포를 따르지 않아도 성립
* ### 최대가능도 추정법(Maximum Likelihood Estimation, MLE)
    * 확률분포마다 사용하는 모수가 다르므로 적절한 통계량이 달라짐
    * 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나로 MLE가 있음

        $\hat{\theta}_{MLE} = \arg\underset{\theta}max L(\theta; X) = \arg\underset{\theta}max P(X \vert \theta)$
    
* ### likelihood
    * 가능도 함수 $L(\theta; X)$는 모수 $\theta$가 따르는 분포가 $X$를 관찰할 가능성.
    * "확률"로 해석하면 안 됨. 주어진 데이터 $X$에 대해 (데이터가 주어져 있는 상황에서) 모수 $\theta$를 변수로 둔 함수.
    * 데이터 집합 $X$가 독립적으로 추출되었을 경우 로그가능도를 최적화함

        $L(\theta; X) = \overset{n}{ \underset{i=1} {\prod} } P(X_i \vert \theta)$

        $\Rightarrow \log L(\theta; X) = \sum_{i=1}^n{P(X_i \vert \theta)}$

    * log likelihood
        * 로그가능도 역시 가능도를 최적화하는 MLE가 됨
        * 데이터 숫자가 수억 단위가 된다면 컴퓨터의 정확도로는 가능도 계산이 불가능 (연산 오차)
        * 데이터가 독립일 경우, 로그를 사용하면 가능도의 곱셈을 로그가능도의 덧셈으로 바꿀 수 있기 때문에 연산이 가능해짐
        * 경사하강법으로 가능도를 최적화할 때 미분 연산을 사용하는데, 로그가능도를 사용하면 연상량이 줄어듦: $O(n^2) \rightarrow O(n)$
        * 대개의 손실함수의 경우 경사하강법을 사용하므로, **negative log-likelihood**를 최적화 
* ### 정규분포에서 MLE

    ||MLE|
    |---|---|
    |추정된 모평균|${\hat{\mu}}_{MLE} = \frac{1}{n} \sum_{i=1}^n X_i$|
    |추정된 모분산|${\hat{\sigma}}_{MLE}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2$|

    * MLE는 불편추정량을 보장하지 않음
    * 통계적 consistency는 보장 가능
* ### 카테고리분포(Multinoulli)에서 MLE

    $\hat{\theta}_{MLE} = \underset{p_1, \dots, p_d}{\arg max} L(\theta; X) = \underset{p_1, \dots, p_d}{\arg max} \log \Big( \overset{n}{ \underset{i=1} {\prod}} \overset{d}{ \underset{k=1} {\prod}} p_k^{X_i,k} \Big)$

    * 카테고리분포의 모수는 다음 제약식을 만족해야 함: $\sum_{k=1}^d p_k = 1$

    * 목적식: $\log \Big( \overset{n}{ \underset{i=1} {\prod}} \overset{d}{ \underset{k=1} {\prod}} p_k^{X_i,k} \Big) = \sum_{k=1}^d \Big( \sum_{i=1}^n X_i,k \Big) \log p_k = \sum_{k=1}^d n_k \log p_k$

    * 라그랑주 승수법을 통해, 목적식과 제약식을 동시에 만족시키는 최적화 문제를 풀 수 있음

    $\Rightarrow \mathcal L(p_1, \dots, p_k, \lambda) = \sum_{k=1}^d n_k \log p_k + \lambda (1 - \sum_{k} p_k)$

    * $0 = \frac{\partial \mathcal L}{\partial p_k} = \frac{n_k}{p_k} - \lambda$ (목적식 미분)
    * $0 = \frac{\partial \mathcal L}{\partial \lambda} = 1 - \sum_{k=1}^d p_k$ (제약식 미분)
    * 각각 미분해서 나오는 이 두 식을 합하면: $p_k = \frac{n_k}{\sum_{k=1}^d n_k}$
        * 카테고리분포의 MLE: **경우의 수를 세어 비율을 구하는 것!**
* ### MLE와 딥러닝
    * 가중치 $\theta = (W^{(1)}, \dots, W^{(L)})$
    * softmax vector는 카테고리분포의 모수 $(p_1, \dots, p_K)$를 모델링
    * one-hot vector로 표현한 정답레이블 $\mathbf y = (y_1, \dots, y_k)$를 관찰데이터로 이용해, 확률분포인 softmax vector의 log likelihood를 최적화할 수 있음

        $\hat{\theta}_{MLE} = \arg \underset{\theta} max \frac{1}{n} \overset{n}{ \underset{i=1} {\sum}} \overset{K}{ \underset{k=1} {\sum}} y_{i,k} \log \Big( MLP_{\theta} (\mathbf x_i)_k \Big)$
* ### 확뷸분포 사이의 거리
    * ML에서 사용되는 손실함수들은, 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포 사이의 거리를 통해 유도함
    * 두 확률분포 $P(\mathbf x), Q(\mathbf x)$가 있을 경우 확률분포 사이의 거리를 계산할 때 사용하는 함수들
        1) 총 변동 거리 (Total Variation Distance, TV)
        2) 쿨백-라이블러 발산 (Kullback-Leibler Divergence, KL)
        3) 바슈타인 거리 (Wasserstien Distance)
* ### KL Divergence
    * 이산확률변수: $\mathbb {KL} (P \Vert Q) = \underset{\mathbf x \in \mathcal X} {\sum} P(\mathbf x) \log \frac{P(\mathbf x)}{Q(\mathbf x)}$
    * 연속확률변수: $\mathbb {KL} (P \Vert Q) = \int_{\mathcal X} P(\mathbf x) \log \frac{P(\mathbf x)}{Q(\mathbf x)}$
    * 분해: $\mathbb {KL} (P \Vert Q) = - \mathbb E_{\mathbf x \sim P(\mathbf x)} [\log Q(\mathbf x)] + \mathbb E_{\mathbf x \sim P(\mathbf x)} [\log P(\mathbf x)]$ (크로스 엔트로피 + 엔트로피)
    * 분류 문제에서 정답레이블을 $P$ 모델의 예측을 $Q$라고 두면, MLE는 KL 발산을 최소화하는 것과 같음
<br><br>