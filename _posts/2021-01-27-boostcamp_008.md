---
title: "Boostcamp AI Tech (Day 008)"
date: 2021-01-27
layout: post
tags: [Naver Boostcamp, daily report]
use_math: true
---

## 1. Pandas

* pandas
    * PANel DAta
    * 구조화된 데이터의 처리를 지원하는 python 라이브러리
    * numpy와 통합해 강력한 spreadsheet 처리 기능 제공
    * 인덱싱, 연산용 함수, 전처리 함수, 데이터 처리 및 통계 분석에 용이
* tabular data
    * attribute(field, feature, column)
    * instance(tuple, row)
    * data(value)
* Series
    * DataFrame 중 하나의 column에 해당하는 데이터의 모음
    * column vector를 표현하는 object
    * subclass of numpy.ndarray
    * index, data, dtype
* DataFrame
    * numpy array-like
    * each column can have a different type
    * row and col index
    * series를 모아서 만든 data table
* loc, iloc
    * loc: locate index (name)
    * iloc: locate index number(0, 1, 2, ...)
* selection with column names
    ```python
    df[['account', 'street', 'state']].head().T
    ```
* map
    ```python
    def change_sex(x):
    return 0 if x == "male" else 1

    df.sex.map(change_sex)
    ```
* useful functions
    * unique()
    * describe(): (숫자형만) 통계치 한번에 출력
    * sort_values()
    * corr(): correlation
    * value_counts()
<br><br>

## 2. 딥러닝의 학습

* neural network
    * 비선형모델
    * d 개의 변수로 p 개의 선형모델을 만들어 p 개의 잠재변수를 설명하는 모델 (한 층에서 다음 층으로 갈 때)
    * $O(n \times p) = X(n \times d) W(d \times p) + b(n \times p)$
         \begin{equation}
        $$\begin{bmatrix} — O_1 — \\ — O_2 — \\ \vdots \\ — O_n — \end{bmatrix} = \begin{bmatrix} — X_1 — \\ — X_2 — \\ \vdots \\ — X_n — \end{bmatrix} \begin{bmatrix} w_{11} & w_{12} & \cdots && w_{1p} \\ w_{21} \\ \vdots \\ w_{d1} & & & & w_{dp} \end{bmatrix} + \begin{bmatrix} \vert & \vert & & \vert \\ b_1 & b_2 & \dots & b_d \\ \vert & \vert & & \vert \end{bmatrix} $$
        \end{equation} $$
* softmax
    * $softmax(O) = \Big( \frac{exp(o_1)}{\sum_{k=1}^pexp(o_k)}, \dots, \frac{exp(o_p)}{\sum_{k=1}^pexp(o_k)} \Big)$
    * 출력 벡터 O에 softmax 함수를 합성하면 확률 벡터가 되므로, 특정 클래스 k에 속할 확률로 해석 가능 (분류 문제에서) (exp: 모든 값을 양수로)
        ```python
        def softmax(vec):
            # 너무 큰 (벡터)값이 들어오면 exp 연산에서 overflow가 발생할 수 있으므로 max를 빼준다
            denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True))
            numerator = np.sum(denumerator, axis=-1, keepdims=True)
            val = denumerator / numerator
            return val
        ```
    * 학습이 아닌 추론을 할 때는, softmax가 아닌 one-hot vector로 (최댓값을 가진 주소만 1로) 출력함
* activation function
    * 신경망: 선형모델과 활성함수(activation function)을 합성한 함수
    * $H = (\sigma(Z_1), \dots, \sigma(Z_n))$
    * 비선형 함수인 활성함수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없음
    * sigmoid, tanh (전통적으로) -> ReLU (딥러닝에서 많이 씀)
* forward propagation
    * $Z^{(1)} = XW^{(1)} + b^{(1)}$
    * $H = \sigma(Z^{(1)})$
    * ...
    * $O = Z^{(L)}$
* 층이 여러 개인 이유
    * 이론적으로는 2층 신경망으로도 임의의 연속함수를 근사할 수 있음(universal approximation theorem)
    * 그러나 층이 깊을수록 목적함수를 근사하는데 필요한 노드의 숫자가 훨씬 빨리 줄어들기 때문에, 더 효율적으로 학습 가능
    * 층이 깊어질수록 복잡한 모델을 만들며, 최적화가 쉬워지진 않음
* backpropagation
    * 역전파를 이용해 각 층에 사용된 파라미터 $\left\{ W^{(\mathcal l)}, b^{(\mathcal l)} \right\}_{\mathcal l = 1}^L$ 학습
    * 역순으로 연쇄법칙(chain-rule)을 통해 gradient vector를 전달
        $\frac{\partial z}{\partial x} = \frac{\partial z}{\partial w} \frac{\partial w}{\partial x}$
    * 역전파 알고리즘은 chain-rule 기반의 자동미분(auto-differentiation) 사용
<br><br>