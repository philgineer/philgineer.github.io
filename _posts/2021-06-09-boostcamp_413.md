---
title: "Boostcamp AI Tech (P4 - Day13)"
date: 2021-06-09
layout: post
tags: [Naver Boostcamp, daily report]
use_math: true
---

## Peer session

### 한 일
- 배철환
   - CSTR 사망 직전
       - 베이스라인 안쓰려고 발악 중..
       - 희한하게 Valid Acc가 0.28 ~ 0.29 정도로 가면 귀신같이 성능 저하
       - 논문에서는 lr이 1로 아주 높게 시작하면서 최종적으로 0.01로 가던데, 이렇게 크게 왜하지?
       - 괄호 penalty가 문제인가?
           - 굳이 GT와 Loss를 계산하는데 2중으로 들어가는건가? 싶기도 한다
       - 시도할 점
           - warmup 주기를 좀 더 천천히 주는 법
           - drop out을 차라리 없애고 weight decay만 주는 법
           - 반대로 weight decay를 안주고 drop out을 그대로 주는법
           - 차라리 CNN이나 모델의 Layer 수를 늘리거나, 채널을 늘리는 법
           - 쌩 CNN보다 차라리 efficientnet으로 쓰면 어떨까 (그놈의 efficientnet 병)
           - 채널 늘리기 (낙점 -> 호성님 지적 : 논문에서의 길이가 25인데 지금 우리 문제는 254니까 dependency가 너무 길어서 그런게 아닌가)
           
    - 궁금한 점
        - 대체 왜 같은 ocr 모델인데 SATRN은 0.7X가 나오는데 왜 왜 왜 왜 CSTR은 안되지?

- 서준배
    - beam을 해보려는데 머리가 나빠서 잘 안됨
    - 시각화 강의 보고 시각화 해보려고함
        - train데이터라도 해보려고하는중....
        - 안되면 그냥 epoch늘려서 성능 박치기라도...

- 윤준호
    - 20에폭 동안 제자리걸음이어서 181 epoch에서 하이퍼파라미터 변경 후 재학습
        - lr: 1e-4 -> 3e-5
        - input_size 128 -> 192 (batch_size 36 -> 16)
        - teacher_forcing_rate는 학습과 함께 조금씩 낮춰봤지만 별로 효과 없는 것 같아서 default로 돌아감 (0.5)
        - 호성님 말씀대로 input_size를 키웠더니 minor한 error들이 덜 나오는 것 같음 (val SA 잘 상승 중)
    - NLP 오타 후처리 paper 읽음
        - [Misspelling Correction with Pre-trained Contextual Language Model](https://arxiv.org/pdf/2101.03204.pdf)
        - BERT pretrained를 사용했으며, 성능 향상이 생각보다 크지 않았음.
        - 수식 data에 있어 BERT 같이 multi task pretrained model이 없기 때문에 현재 프로젝트에 적용하기 힘들 것 같음.

- 조호성
    - SATRN locality aware feedforward 적용
    - 논문보니 hidden dim -> 512, filter dim -> 4 * hidden dim
      encoder layer 12, decoder layer 6  -> 너무 크다
      Cycle LR -> max_lr : 3e-4, step:250000
    - (64, 256) attention모델 20에폭 기준으로 조금 더 잘나오는듯
    - satrn adaptive + deformable은 망한듯..
        - 성능이 안나오나요? 
        - 기존보다 떨어졌어요.. 마지막으로 한번더 시도해볼까하는중 :disappointed_relieved: 

- 임기홍
    - 뭐가 틀렷을까? 오답 스캔중.
        - 지금 h //2 다시 하면서 확인중.
        - 오답을 보면서 생각을 해봐야겠음.
        - SATRN은 왜 90도 회전한 이미지를 왜 인식을 잘 못할까. train set에서는 200 ~ 300 여개의 90도 회전이 있다고 하는데. transform에 추가를 해서 이부분도 학습을 시켜야하는가 
        - 비슷한 문자를 잘 인식못함.. ex) 1 vs 4    - vs (나누기표시)
        - 작은점 i처럼 위에 작은점 인식을 잘하는방법이 resize를 늘리는거 밖에 없을까
    - Symbol 학습은 Mathematical expression이 아니어서 학습을 고려 하지 않았는데 비슷한 Symbol에서 오답이 많이 발생하는것을 보고 학습을 해야해야할까? 라는 고민이 생김.

- 김현우
    - 가로-> 세로로 변환, rotate augmentation, 100 에폭 학습중 (준배님 코드)
    ![](https://i.imgur.com/H5K977l.png)
    - 네트워킹 데이 발표 자료 내용 고민중

### 멘토님의 한마디
- 남세동 대표님
    - [유튭인터뷰](https://www.youtube.com/watch?v=kMGEpIYPCiM&ab_channel=EO)
    - 스노우 카메라
    - 면접용 자가 테스트 도움이 안된다. 
- Chrome 데이터셋을 Psuedo 라벨링을 해야 할지?
    - 데이터 전처리 후 학습으로 가능할 듯
- 모델이 잘못되어 있을때 어떤 방식으로 디버깅을 해야 하나요?
    - 디버깅이 보다 실험 방식으로 접근해야 함
    - 오픈 소스를 적용하여 제대로 동작하지 않으면 버려야 함
    - 오픈 소스를 적용하여 동작하면 기능을 하나씩 추가하면서 실험
- 네트워킹데이
    - 비슷한 대회로 발표자료는 다 비슷할듯하다
    - 스토리(포장)를 잘 짜야할지도
        - 성능위주의 발표보다는 서사를 정해서 진행하기
        - 모델의 탄생과 죽음
- 담주 금요일 5시! 
    - 네트워킹 사전에 사전 준비

### 최종 버전
- 조호성 : seed 222, size (64, 256)
- 배철환 : seed 666, size (49, 196) + cstr 마지막 호흡기!
- 김현우 : seed 1234 , rotate, size (기본), 세로, AdamW  -> 가로 적용(0.75), 준배님 코드 적용 
- 임기홍 : seed 111 , size (128,384)
- 서준배 : seed 1097 + size(기본) + SATRN + adaptivePE + 세로->가로(0.75) + adamW(잘된다고함) + dim 최대로 + aida(새로운데이터셋 가능하면 추가)
- 윤준호 : seed 1138, size (192, 192), adaptivePE, CNNdepth32, augmentations, 세로 가로 0.75
-> 성능이 좋은 모델 4개를 앙상블하여 제출(시간이 허용하면 많이)
```
LoadEvalDataset, LoadDataset 둘 다 넣기 

def __getitem__(self, i):
        item = self.data[i]
        image = Image.open(item["path"])
        width, height = image.size
        if (width/ height) < 0.75:
            angle = 90
            image = image.rotate(angle, expand=True)
            
```

### 정리
- CSTR이 안나오는이유는?
    - 같은 모델이고 더 좋은모델이라 되어있는데 왜 안될까
        - 로직이 달라서? 근데 train도 떨어짐
    - 이미지로부터 250개(라텍스 토큰갯수)를 봐야함
        - 기존논문은 알파벳+숫자만 판단
        - 너무 많아서 그런게 아닐까
- dimension(서준배 최대로 늘렸던 부분)
- 금요일 부터 네트워킹데이 문서 작성 시작
```
SATRN:
  encoder:
    hidden_dim: 300 
    filter_dim: 700 # 600 
    layer_num: 6
    head_num: 8
  decoder:
    src_dim: 300
    hidden_dim: 200 # 128
    filter_dim: 600 # 512
    layer_num: 3
    head_num: 8
```
- 앙상블
    - 데이터 나누기
        - kfold or seed
    - 크기 다르게
        - multi scale
- 엔지니어 듣는 입장으로 발표 준비