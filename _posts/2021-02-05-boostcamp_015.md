---
title: "Boostcamp AI Tech (Day 015)"
date: 2021-02-05
layout: post
tags: [Naver Boostcamp, daily report]
use_math: true
---

[![Peer Session Badge](https://img.shields.io/badge/Peer%20Session-CC527A?style=flat)](../peer_session/day015.html)

## 1. Generative models

* ### 생성모델이 할 수 있는 것
    * Generation (sampling)
        * sample $x_{new} \sim p(x)$
        * implicit model (VAE, GAN처럼 생성만 하는 모델)
    * Density estimation (anomaly detection)
        * $p(x)$ should be high
        * explicit model (확률값까지도 얻어낼 수 있는 모델)
    * Unsupervised representation learning (feature learning)
        * 이미지의 공통적 특징들학습
* ### Basic distributions
    * Bernoulli dist.
        * $D$ = {Heads, Tails}
        * Specify $P(X = Heads) = p$, then $P(X = tails) = 1 - p$
        * $X \sim Ber(p)$
    * Categorical dist.
        * $D$ = {1, ...,  m}
        * Specify $P(Y = i) = p_i$ such that $\sum_{i=1}^m p_i = 1$
        * $Y \sim Cat(p_1, \dots , p_m)$
* ### Parameter 개수
    * $p(x1, \dots, x_n) = p(x_1)p(x_2 \vert x_1)p(x_3 \vert x_1, x_2) \dots$
        * $p(x_1)$: 1 parameter
        * $p(x_2 \vert x_1$: 2 parameters ($p(x_2 \vert x_1 = 0), p(x_2 \vert x_1 = 1)$)
        * $p(x_3 \vert x_1, x_2)$: 4 parameters
        * total: $1 + 2 + 2^2 + \dots 2^{n-1} = 2^n - 1$
        * n개의 independent binary pixel을 가진 이미지의 파라미터 수로도 해석할 수 있음
    * Markov assumption 적용 시 획기적으로 감소
        * suppose $X_{i+1} \perp \{ X_1, \dots, X_{i-1} \} \vert X_i$
        * $X_{i+1}$(미래)의 조건부확률분포는 $X_i$(현재)가 주어졌을 때, 이전의 모든 과거 $X_1, X_2 \dots, X_{i-1}$ 와 독립
        * $p(x1, \dots, x_n) = p(x_1)p(x_2 \vert x_1)p(x_3 \vert x_2)p(x_4 \vert x_3) \dots$
        * 총 파라미터 수: **2n - 1** (exponential reduction!)
* ### Auto-regressive models
    * Markov chain 활용해 이전의 n개(AR-n model)나 1개(AR-1 model)를 고려함
    * chain rule로 joint distribution을 나눔
    * 랜던 변수들의 순서(order)가 필요함
        * 이미지(2차원)의 순서(1차원)를 매기는 방법에 따라 성능이 달라짐
    * 어떤 식으로 conditional indepence를 주는 지에 따라 전체 모델의 structure가 달라짐
* ### NADE (Neural Autoregressive Density Estimator)
    * i번째 픽셀을 1,2,...,i-1번째 필셀까지에 dependent하게
    * 매번 뉴럴넷 층으로 보냄 (층의 사이즈가 점차 커짐)
    * explicit model로, generation뿐만 아니라 확률(density) 계산도 가능
        * joint dist. 이용해 $p(x_i \vert x_{1:i-1})$ 계산 가능
    * continuous variables을 모델링할 경우, 마지막 layer에 Gaussian mixture model 활용해 연속 dist 만들 수 있음
* ### Pixel RNN
    * use RNN to define an auto-regressive model
    * $p(x) = \prod_{i=1}^{n^2}$ (prob i번째 R) (prob i번째 G) (prob i번째 B)
    * $p(x) = \prod_{i=1}^{n^2} p(x_{i,R} \vert x_{<i})p(x_{i,G} \vert x_{<i},x_{i,R})p(x_{i,B} \vert x_{<i},x_{i,R},x_{i,G})$
    * ordering에 따라 두 종류
        * Row LSTM
        * Diagonal BiLSTM
<br><br>

## 2. Generative models - Latent variable models

### 2.1. VAE (Variational Auto-Encoder)

* ### Variational inference
    * 목적: observation 주어졌을 때, 관심 있는 random variables의 확률 분포, 즉 posterior dist.를 찾는 것
    * 방법: optimize(최적화, 근사) the **variational dist.** that best matches the posterior dist.
        * Posterior dist.: $p_{\theta}(z \vert x) \Rightarrow$ Decoder
        * Variational dist.: $q_\phi(z \vert x) \Rightarrow$ Encoder
    * loss function: KL divergence

        $\ln p_{\theta}(D) = \mathbb E_{q_{\phi}(z \vert x)} [\ln p_{\theta}(x)]$

        $= \mathbb E_{q_{\phi}(z \vert x)} [\ln \frac{p_{\theta}(x,z)}{q_{\phi}(z \vert x)}] + D_{KL} \Big( q_{\phi} (z \vert x) \Vert p_{\theta}(z \vert x) \Big)$

        $= \text{ ELBO } + \text{ Objective}$

    * ELBO
        * Evidence Lower BOund
        * Objective는 알 수 없기 때문에, tractable한 ELBO 값을 증가시키는 방향으로 (minimize objective via maximizing ELBO)
        * Decompose ELBO

            $\mathbb E_{q_{\phi}(z \vert x)} [\ln \frac{p_{\theta}(x,z)}{q_{\phi}(z \vert x)}] = \int \ln \frac{p_{\theta}(x \vert z) p(z)}{q_{\phi}(z \vert x)} q_{\phi}(z \vert x) dz$

            $= \mathbb E_{q_{\phi}(z \vert x)} [q_{\phi}(x \vert z)] - D_{KL} \Big( q_{\phi} (z \vert x) \Vert p(z) \Big)$

            $= \text{Reconstruction term} - \text{Prior fitting term}$

        * Reconstruction term: minimize the reconstruction loss of an auto-encoder
        * Prior fitting term: enforce the latent dist. to be similar to the prior dist.
* VAE의 한계
    * intractable model (implicit, hard to evaluate likelihood)
    * prior fitting term must be differential(미분가능) $\rightarrow$ 따라서 대부분 가우시안 prior를 사용
* Adversarial auto-encoder
    * GAN을 사용해 latent 분포들 사이의 분포를 맞춰줌
    * VAE보다 대부분 성능이 잘 나옴
<br><br>

### 2.2. Generative Adversarial Network (GAN, 2014)

* two player minimax game bw **G**enerator and **D**iscriminator
    
    $\underset{G} {\min} \underset{D} {\max} V(D, G) = \mathbb E_{x \sim p_{data}(x)}[\log D(x)] + \mathbb E_{z \sim p_{z}(z)}[\log (1 - D(G(z)))]$

    * Optimal discriminator $D^*_G(x) = \frac{P_{data}(x)}{p_{data}(x) + p_G(x)}$
    * $V(G,D^{\ast}_G(x)) = \dots = 2D_{JSD}[P_{data},P_G] - \log 4$

        $=$ two Jenson-Shannon Divergence (을 최소화)
* DCGAN
    * image domain으로 활용함 (Deep Convolutional GAN)
* Info-GAN
    * 단순히 True/False 뿐만 아니라, class를 랜덤하게 같이 넣어줘서 학습
* Text2Image
* Puzzle-GAN
* CycleGAN
    * cycle-consistency loss
    * 이미지에서 두 도메인을 서로 바꿀 수 있음
* Star-GAN
* Progressive-GAN
<br><br>