---
title: "Boostcamp AI Tech (Day 011)"
date: 2021-02-01
layout: post
tags: [Naver Boostcamp, daily report]
use_math: true
---

My assignment: [MLP pytorch](https://colab.research.google.com/drive/1dv68QBIzOeCsYzlK5b1w5cz5nTfCK4wy?usp=sharing)
<br><br>

## 1. 베이즈 통계학

* ### 베이즈 정리
    * 조건부 확률을 이용해 유도
        * $P(A \cap B) = P(B)P(A \vert B)$
        * $P(B \vert A) = P(B) \frac{P(A \vert B)}{P(A)}$
    * A라는 새로운 정보가 주어졌을 때, P(B) given A를 계산하는 방법 제공
    * 조건부확률을 이용해 정보를 갱신하는 방법
    * $P(\theta \vert \mathfrak D) = P(\theta) \frac{P(\mathfrak D \vert \theta)}{P(\mathfrak D)}$
        * $P(\mathfrak D \vert \theta)$: 가능도(likelihood). 현재 주어진 파라미터(모수, 가정)에서 이 data가 관찰될 확률
        * $P(\mathfrak D)$: Evidence. data 자체의 분포
        * $P(\mathfrak D) = P(\mathfrak D \cap \theta) + P(\mathfrak D \cap \theta^c)$
        * 이를 통해 $P(\theta) \rightarrow P(\theta \vert \mathfrak D)$로 업데이트
        * 즉 사전확률(prior)에서 사후확률(posterior)로 업데이트
        * 계산한 사후확률을 다시 새로운 사전확률로 사용해 사후확률 갱신 가능
* 조건부 확률과 인과관계
    * 조건부 확률은 유용한 통계적 해석을 제공하지만, 인과관계(causality) 추론에 함부로 사용하면 안 됨
    * 인과관계는 데이터 분포의 변화에 강건한 예측모형을 만들 때 필요
    * 조정(intervention)을 통해 중첩요인(confounding factor)의 효과를 제거하고, 원인에 해당하는 변수만의 인과관계를 계산해야 함 (제거하지 않으면 spurious correlation이 나옴)
    * Simpson's paradox 주의
<br<br>

## 2. Deep learning - historical review

* ### Key components of deep learning
    * The "**DATA**" that the model can learn from
    * The "**MODEL**" how to transfrom the data
    * The "**LOSS**" function that quantifies the badness of the model
    * The "**ALGORITHM**" to adjust the parameters to minimize the loss
* ### History
    * 2012: AlexNet
    * 2013: DQN (Deep Q-Net)
    * 2014: Encoder/Decoder, Adam
    * 2015: GAN, ResNet
    * 2017: Transformer (Attention is all you need)
    * 2018: BERT (Bidirectional Encoder Representations from Transformers) (fine-tuned NLP models)
    * 2019: BIG language models (GPT-X)
    * 2020: Self-supervised learning (SimCLR)
<br><br>

### (PyTorch 특징)

* Numpy + AutoGrad + Function
    * Numpy 구조를 가지는 Tensor 객체로 array 표현
    * 자동미분을 지원하여 DL 연산을 지원
    * 다양한 형태의 DL을 지원하는 함수와 모델을 지원
<br><br>

## 3. NN & MLP

* ### Neural Networks
    * Previous definition:
        * "Computing systems vaguely inspired by the biological neural networks that constitute animal brains"
    * 시작은 그랬지만, 현재 역전파 등의 알고리즘들이 뇌의 활동과 관계 있는지는 회의적
    * Current definition:
        > "**Function approximators** that stack **affine** transformations followed by **nonlinear** transformations"
* ### Linear NN
    * Data: $\mathcal D = \{ (x_i, y_i)\}_{i=1}^N$
    * Model: $\hat y = wx + b$
    * Loss: $loss = \frac{1}{N} \sum_{i=1}^N (y_i - \hat y_i)^2$
    * Backprop: $\frac{\partial loss}{\partial w} = \frac{\partial}{\partial w} \frac{1}{N} \sum_{i=1}^N (y_i - wx_i - b)^2$

        $= - \frac{1}{N} \sum_{i=1}^N -2(y_i - wx_i - b)x_i$

    * Update: ($\eta$: stepsize)
        * $w \leftarrow w - \eta \frac{\partial loss}{\partial w}$
        * $b \leftarrow b - \eta \frac{\partial loss}{\partial b}$
* ### More layers
    * $y = W_3^Th_2 = W_3^T \rho (W_2^TX h_1) = W_3^T \rho (W_2^T \rho (W_1^TX))$
    * $\rho$: nonlinear transform
* ### Activation functions
    * ReLU (Rectified Linear Unit)
    * sigmoid
    * tanh (Hyperbolic Tangent)
* ### Universal approximation theory
    * 히든 레이어가 1개 있는 신경망의 표현력은 일반적인 continuous function들을 포함함
    * 존재성에 대한 증명 (어떻게 찾는지까지에 대한 이론은 아님)
* ### Loss functions
    * Regression task: (주로) $MSE = \frac{1}{N} \sum_{i=1}^N \sum_{d=1}^D (y_i^{(d)} - \hat y_i^{(d)})^2$
    * Classification task: (주로) $CE = -\frac{1}{N} \sum_{i=1}^N \sum_{d=1}^D y_i^{(d)} \log \hat y_i^{(d)}$ 
    * Probabilistic task: (주로) $MLE = \frac{1}{N} \sum_{i=1}^N \sum_{d=1}^D \log \mathcal N (y_i^{(d)} ; \hat y_i^{(d)}, 1)$ (=MSE)
<br><br>