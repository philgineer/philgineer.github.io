---
title: "Boostcamp AI Tech (P4 - Day09)"
date: 2021-06-03
layout: post
tags: [Naver Boostcamp, daily report]
use_math: true
---

## Peer session

# 6월 3일

### 한 일
 - 김현우
    - 저장 경로를 /opt/ml/input/data 로 변경하면 서버 저장 잘 됨 (다행)-> TTA inference 제출하면 실패
    - Atttention 에서 layer 를 변경하면 성능이 떨어짐
 - 배철환
     - SRN 사망..인 줄 알았으나 다른 소스(unofficial)로 다시 짜봄
         - 이거 안되면 진짜 접음
         - 다행인 거는 배치 사이즈 50, num workers 8로, 시간 줄어듬 (1시간 -> 30분)
 - 조호성
     - 서버저장 이슈는 버전 업그레이드 하지않는걸로 하면 되는듯
     - 근데 deformable 은 torchvision 업그레이드가 필요해서 attention 돌릴때 에러가 남 (SE-Aster 해보려했는데)
     - 현재 CSTR 구현중
 - 서준배
     - 원본 SATRN(pytorch satrn, tf satrn)보면서 비교
         - dim 변경시 오류
     - 원본과 비교하는데 폴더 구조가 왜이를까
     - positional encoder2d -> adaptive 2d 로 바꿔봄
         - 입력 채널수가 다름
 - 임기홍
     - AREA 25%짜리 0.5이상까지는 나옴. AREA 50%도 시도해보는데 잘나왔으면 좋겠음.(학습도가 빨라서..)
     - AREA 50% (width*0.7,height*0.7) 까지는 batch_size 72까지는 가능함.
     - 작은 이미지로 빠르게 많이 학습 vs 큰 이미지로 느리게 적게 학습.
     - AIDA DATASET 쓰고 싶은데 규칙변환
         - aida = a_{0} = mmm*10 , -boostcamp a _ { 0 } = m m m * 1 0
             - Token단위로 띄어쓰기 하고 Token이 아닌부분은 character 별로 띄어쓰기를 해야하는데 ....Token을 어떻할까요..?
- 윤준호
    - augmentation + cosine annealing 100에폭까지 sentence accuracy 단조증가하며 잘 학습함. 서버 저장까지 됐지만 제출 시 score 0 나옴.
        - dataset.py에서 Image.imread가 아니라 cv2.imread로 바꿔줬었기 때문에 생긴 문제로 짐작. (Albumentation 사용 시 필수)
    - dataset.py를 수정해 제출 시 정상적인 결과 나오게 해볼 예정.
    - SATRN.py에서 CNN Dense block depth를 paper에서 제시하는 만큼 (2배) 늘려볼 예정. (size error 해결에 초점) 

### 해볼 만한 것들
- 외부 데이터 사용
    - 조심할 점: 같은 수식이라도 다른 latex 문법으로 라벨링되는 여러 경우의 수가 있을 수 있는데, 주어진 10만개 데이터셋은 특정 규칙에 의해 만들어졌다고 했으니 외부 데이터셋도 사용하기 전에 확인 필요 (규칙 차이는 생각보다 간단합니다. 변환이 문제입니다.)
- Post-processing
    - 한 두 글자 오탈자 같이 minor한 차이로 sentence acc에 0점을 받는 case들을 줄여줄 수 있는 새로운 post-processing 네트워크 

































