---
title: "Boostcamp AI Tech (P4 - Day02)"
date: 2021-05-25
layout: post
tags: [Naver Boostcamp, daily report]
use_math: true
---

## Peer session

### 오늘 한 일 
* 윤준호 (나)
    * Open CV 로 색깔, 형광펜, 종이 질감을 효과적으로 통일할 수 있는 필터 실험 ([참고 블로그: 경계 검출 필터](https://bkshin.tistory.com/entry/OpenCV-18-%EA%B2%BD%EA%B3%84-%EA%B2%80%EC%B6%9C-%EB%AF%B8%EB%B6%84-%ED%95%84%ED%84%B0-%EB%A1%9C%EB%B2%84%EC%B8%A0-%EA%B5%90%EC%B0%A8-%ED%95%84%ED%84%B0-%ED%94%84%EB%A6%AC%EC%9C%97-%ED%95%84%ED%84%B0-%EC%86%8C%EB%B2%A8-%ED%95%84%ED%84%B0-%EC%83%A4%EB%A5%B4-%ED%95%84%ED%84%B0-%EB%9D%BC%ED%94%8C%EB%9D%BC%EC%8B%9C%EC%95%88-%ED%95%84%ED%84%B0-%EC%BA%90%EB%8B%88-%EC%97%A3%EC%A7%80))
    * Task가 동일한 논문 탐색 & 읽어볼 예정
        * [Translating Math Formula Images to LaTeX Sequences Using Deep Neural Networks with Sequence-level Training](https://paperswithcode.com/paper/translating-mathematical-formula-images-to)
        * [Image to Latex](http://cs231n.stanford.edu/reports/2017/pdfs/815.pdf)
    * [수식OCR tutorial](https://blog.ayoungprogrammer.com/2013/01/part-3-making-ocr-for-equations.html)를 참고해 Tesseract로 수식 인식을 테스트해볼지 고민 중

* 서준배 
    * 논문의 Abstract 를 리뷰
        * [논문](https://github.com/Ykmoon/scene-text-detection-recognition)에서 일부 4개 훓어봄
    * 수식 인식보다는 텍스트 위주 논문
    * 휘어진 글자를 학습하는 논문
        * Synthetically Supervised Feature Learning for Scene Text Recognition
        * ESIR: End-to-end Scene Text Recognition via Iterative Image Rectification
    * ACE Loss 
        * CTC, Attention기반에서 좋다고 소개
        * Aggregation Cross-Entropy for Sequence Recognition
    * 평가방식
        * What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis(네이버에서 만듬)
    * OCR 논문은 찾아야 하지 않을까?

* 배철환
    * 데이터 EDA 하면서 어떤 Agumetation 적용할지 탐색
        * Transform : RandomBrightnessContrast + ToGray + Black & White Threshold로 RGB를 흑백으로 처리
            * 꽤 잘나오나 역시 아웃라이어 (00678번 JPG)에는 좋지 않다
    * 비정상적인 데이터는 어떻게 할까? 데이터를 교정해야 할 것 인가?노이즈로 판단할 것인가?
        * 개인적으로는 노이즈로 생각한다
            * 도메인을 생각했을 때 아무리 생각해도 적절한 Flip은 아니라고 생각한다
    * 데이터 flip 을 어떻게 할 것인가? 사람이 직접 교정해야 하는지? 
    * 근데 그런 outlier를 처리한다 하더라도 test에 그런 데이터가 들어오면 어떻게 해야하는지?
    * 그래서, 이미지를 생으로 넣어서(no augmentation) 레벨을 잘 판단하면 어느정도 augment에 강건하지 않을까?에 대한 가설
        * 코드 작성 중
        * 만약 잘된다면, 굳이 augment를 주지 않더라도 상관없는 결론? -> 그렇다고 아예 안줘야 하나?
            * 그런데 level을 잘 잡는다고 OCR을 잘하는 거랑 상관이 있나?
        * 만약 잘안된다면, augment를 줘야 한다는 결론, 그러면 어떤 것을 줘야 하나?

* 조호성
    * EDA 데이터 탐색
    * 검증 방법을 어떻게 해야 할 것인가? 
        * 손글씨 데이터 반, text 데이터 반
        * level도 고려를 해야 할 것인가? Seq len을 고려 할 것인가?
        * 토큰 출현 빈도?
    * Scene Text Recognition SOTA SRN 논문 리뷰중
    
* 임기홍
    * Text 데이터의 내용으로 자료의 출처를 찾고 있었음
    * 수학에서 자주 나오지 않는 용어가 나옴
    * 대학원, 양자역학 내용이 포함되어 있음
    * 강의에 나온 논문 위주로 나옴
    * 이상치 데이터가 많이 나옴


 
 * 김현우
     * train데이터 fold시키기
     * level은 아직은 신경안씀
         * 복잡도로 따지면 길이 or level
         * 짧아도 어려운 수식이 존재하는 예외 등 고려할게 많다.
     * 손글씨와 이미지 분류해서 데이터 나누기
     * test데이터가 없어서 아직 ratio(가로세로)의 기준을 정하기 어려움
         * ratio가 1이 아닐때 처리에대해 판단이 아직은...
     * 서버 용량이 작은것 같아요.

### 계획
- 손글씨와 프린트가 너무 격차가 심하다.
    - 손글씨는 사람마다 다 다른다.
    - 프린트는 폰트차이라도 어느정도 인식이 된다.
    - 내일 베이스코드를 보고 쪼갤지 아닐지 판단.
        - 어떤 모델인지도 보이지 않음
    - 프린트인지 손글씨인지 여부에 따라 분리해서 학습?
        - 예상으로는 프린트가 좀더 성능이 좋을것
    - 내일 확인하고 생각해보기
- 내일 코드나오면 리뷰하기
- 모델 아키텍처 아이디어
    - Train
        - 모델 1: 손글씨/프린트사진 classification 학습
        - 모델 2-1: 손글씨 데이터에 대한 image to latex 학습
        - 모델 2-2: 프린트사진 데이터에 대한 image to latex 학습
    - Inference
        - 테스트 데이터(image) -> 모델 1 -> 손글씨인지 프린트사진인지 예측
        - 예측 결과에 따라 모델 2-1, 2-2 중 하나로 latex 예측

### 참고 자료 
* [Aster Pytorch](https://github.com/ayumiymk/aster.pytorch)
* [수식 인식기 논문](https://arxiv.org/pdf/1908.11415.pdf)
* [수식 인식기 논문 해당 paperswithcode](https://paperswithcode.com/paper/translating-mathematical-formula-images-to)
* [Open CV 필터들](https://bkshin.tistory.com/entry/OpenCV-18-%EA%B2%BD%EA%B3%84-%EA%B2%80%EC%B6%9C-%EB%AF%B8%EB%B6%84-%ED%95%84%ED%84%B0-%EB%A1%9C%EB%B2%84%EC%B8%A0-%EA%B5%90%EC%B0%A8-%ED%95%84%ED%84%B0-%ED%94%84%EB%A6%AC%EC%9C%97-%ED%95%84%ED%84%B0-%EC%86%8C%EB%B2%A8-%ED%95%84%ED%84%B0-%EC%83%A4%EB%A5%B4-%ED%95%84%ED%84%B0-%EB%9D%BC%ED%94%8C%EB%9D%BC%EC%8B%9C%EC%95%88-%ED%95%84%ED%84%B0-%EC%BA%90%EB%8B%88-%EC%97%A3%EC%A7%80)
* [im2markup 깃허브](https://github.com/harvardnlp/im2markup/)
* [Image to Latex 논문](http://cs231n.stanford.edu/reports/2017/pdfs/815.pdf)

