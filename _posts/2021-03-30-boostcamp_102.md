---
title: "Boostcamp AI Tech (P1 - Day02)"
date: 2021-03-30
layout: post
tags: [Naver Boostcamp, daily report]
use_math: true
---

## 개인 Project 진행 상황
* Dataset 통합 파일 (.py) 임시 완성
```python
class MyDataset(Dataset):
    def __init__(self, csv_file, root_dir, is_Train=True, input_size=input_size, transform=None):
        """
        Args:
            csv_file (string): csv 파일의 경로
            img_dir (string): 모든 이미지가 존재하는 디렉토리 경로
            transform (callable, optional): 샘플에 적용될 Optional transform
        """
        super().__init__()
        self.root_dir = root_dir
        self.transform = transform
        
        csv = pd.read_csv(csv_file)
        img_path = []
        path = csv['path']
        
        for person in path:
            images = [root_dir + person + '/' + image for image in os.listdir(root_dir + person) if not image[:1] == '.']
            for image in images:
                img_path.append(image)
        
        self.img_path = img_path
        self.combination = [(m, g, a) for m in ['m', 'i', 'n'] for g in ['male', 'female'] for a in [0, 1, 2]]

    def __len__(self):
        return len(self.img_path)

    def __getitem__(self, idx):
        img_name = self.img_path[idx]
        mask = img_name.split('/')[-1][:1]
        features = img_name.split('/')[-2].split('_')
        gender = features[1]
        age = int(features[3]) // 30
        
        for i, (m, g, a) in enumerate(self.combination):
            if mask == m and gender == g and age == a:
                target = i
        
        img = cv2.imread(img_name)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = img/255.
        img = img.reshape(3, 512, 384)
        
        return img, target
```
* Transform, data loading, trainig을 포함한 전체 사이클 임시 완성
* 서버와 연동된 ssh, vscode 통합환경 구축
<br><br>

## 생각해볼 부분
* 단순 크로스엔트로피 말고, f1 score 처럼 data imbalance를 극복하는데 도움될 수 있는 loss 설정?
* transform 종류와 방법 선정, 실험
* 학습 시간을 줄일 수 있는 방법 고찰해볼 것 (num_workers, batch_size, data loader 등)
<br><br>

## Peer session
* 60세 이상 데이터 적음
    * 조원 Q: 노인 데이터에 대해서만 augmentation?
        * my A: augmentation을 동등하게 하지 않고, 클래스별로 다르게 하게 되면 오히려 데이터가 왜곡될 것 같다. 모든 클래스에 동등하게 해야될 거라고 생각함.
    * 조원 idea: bootstrapping
        * 같은 데이터 복원 추출
    * My idea: cutmix에 착안해, 노인 사진 + 마스크 합성으로 데이터 추가 생성
    * My source: [불균형 데이터 분류 (TF 튜토리얼)](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data)
        * 클래스 가중치
        * 오버샘플링 (부트스트래핑)
        * bias 초기화 방법 다르게 (init_bias = np.log(pos/neg))
<br><br>

## 강의
* ### Data processing
    * 주어진 Vanilla Data $\rightarrow$ 모델이 원하는 형태의 Dataset
    * Pre-preocessing
        * 도메인, 테스크 마다 다름
        * 새로운 방법을 도입할 때는 성능이 좋아지는지 실험을 통해 비교 필수
* ### Generalization
    * Bias & Variance
        * underfitting: high bias
        * overfitting: high variance
    * 일반화 성능 높이는 방법들
        * Train set 일부분을 Validation set으로 활용
        * Data augmentation
            * case, state의 다양성
            * torchvision.transforms, albumentations
    * 유의할 점
        * 무조건 적용 가능한 마스터키는 존재하지 않는다
        * problem을 깊이 관찰해, 어떤 기법을 적용하면 어떠한 다양성을 가질 수 있을지 가정한 후, **실험으로 증명**해야 한다
* ### Data feeding
    * data generator의 처리 속도 (batch / s) 와 model의 처리 속도를 맞춰서 최적화
    * 순서 주의: resize, rotate를 순서만 다르게 해도 처리 속도 차이가 많이 날 수 있음
    * dataset과 data loader 분리
        * dataset: vanilla 데이터를 원하는 형태로 출력하는 클래스
        * data loader
            * dataset을 더 효율적으로 사용하기 위해 사용 (num_workers, drop_last 등)
            * 다른 dataset을 input으로 받아도 됨
<br><br>