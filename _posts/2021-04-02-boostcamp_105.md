---
title: "Boostcamp AI Tech (P1 - Day05)"
date: 2021-04-02
layout: post
tags: [Naver Boostcamp, daily report]
use_math: true
---

## 개인 Project 진행 상황
* ### 수정할 부분
    * torch.seed도 고정 (random만 했었음)
    * (정리 예정)
<br><br>

## 생각해볼 부분 (continued)
* Gradient accumulation & batch_size를 늘리는 것 - 정확히 동일한 것인지? 차이가 있는지?
* oversampling
    * weighted ramdom sampler
* class, loss 가중치
    * class-weights
    * focal loss
* 노인 외부 데이터 수집, mask 생성 후 feed
* efficientnet b0 외에 더 데이터 표현에 적합한 모델 search
* cutmix?
* torch.seed
* 터미널 환경에서 hyper params 바꿔가며 학습 테스트할 수 있도록 자동화
<br><br>

## Peer session
* (규진님) torch.cuda.amp로 mixed precision 적용해보기(6줄 추가로 학습시간 10%-20% 개선)
* (규진님) 3 조건 별로 end-to-end 모델 만들어서 했는데, 18개 클래스를 한 번에 학습하는 게 더 성능이 잘 나오는 걸 확인한 분을 본 적 있음
* focal loss, mixup 시도해볼 예정
* Gradient accumulation과 batch_size 늘리는 것의 차이
    * 전자의 경우, grad 변화가 여러 차례에 걸쳐 변화한 후 업데이트
    * 후자의 경우, batch 단위로 한번에 grad 변화 후 즉시 업데이트
    * 추가적으로 batch size를 키우면, batch norm을 할 때 더 많은 샘플을 가지고 normalize 하기 때문에 더 좋은 일반화가 가능하다
* imbalanced data를 학습할 때 fc layer의 bias 항을 적은 확률의 클래스 쪽으로 치우치게 줌으로서 학습에 도움되는 방법이 있음 (어제자 TF tutorial 참조)
    * 하지만 예시 코드의 경우 binary classification이기 때문에 bias만 가지고 인위적 조작이 가능한 반면, 18개의 클래스가 있는 현 프로젝트의 경우 18차원 공간에서 1차원 bias를 클래스에 맞게 줄 수가 없음. class-weights 방법이 비슷한 기능을 할 것 같음.
<br><br>

## 강의
* (정리 예정)
<br><br>