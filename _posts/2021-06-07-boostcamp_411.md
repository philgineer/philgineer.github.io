---
title: "Boostcamp AI Tech (P4 - Day11)"
date: 2021-06-07
layout: post
tags: [Naver Boostcamp, daily report]
use_math: true
---

## Peer session

### 한 일

- 배철환
    - CSTR 테스팅 중
        - 왜 오버피팅이 났을까
            1. 학습률이 너무 크다
                - 근데 초반 학습은 엄청 잘된다. 그러면 시작 학습률을 0으로 두고,
                - CosineAnnealingWarmup 주기를 빠르게 주면서 max값을 1로 설정하면 되지 않을까
            2. drop-out이 없다
                - 함부로 모델을 건드리기는 뭣해서, 파라미터에 dropout이 있는 녀석들만 0.1로 줬다
            3. weight-decay를 안줬다
                - 원래 SATRN이 1e-4인데, 조금 더줘서 1e-3으로 줬다
            4. 또 생각하면 기홍님의 Aida Dataset을 추가하면 괜찮을거 같다는 멘토님의 첨언
            비슷한 맥락으로 augmentation도 넣으면 좋을거 같다
        - beam search를 적용할 수 있을까?
            - 검색해보니 RNN 계열만 beam search를 쓰던데, CSTR은 CNN 계열이라 못쓸까??
        - 다른 걸 적용할 수 있을까?
            - 그나마 hyperparameter 조정??
            - 아니면 단계별 학습??
- 윤준호
    - CNN depth 32, cosine annealing, augmentations 모델 조금씩 튜닝하면서 계속 학습 진행 중
        - 100 에폭부터: teacher forcing 0.5 -> 0.4, lr 5e-4 -> 3e-4, resize 2/3 없애고 풀사이즈, gaussian noise 없앰
        - 136 에폭 이후로 val SA 튀어서 teacher forcing 0.4 -> 0.3, lr 3e-4 -> 1e-4
        - ![](https://i.imgur.com/VJqhdpr.png)

- 김현우
    - 가로 이미지 세로로 변형 : 0.051 상승
    - ASTRN 튜닝하면서 실험해야 겠음

- 조호성
    - CSTR 실패. 기존의 코드 적용한거는 22에폭까지 0나왔고, 논문 구현으로 코드 작성한거는 0.1을 넘지못함.

- 서준배
    - adpative 성능이 괜찮게 나옴
    - hidden_dim, filter_dim 소폭 증가해서 진행중
        ```python
        SATRN:
          encoder:
            hidden_dim: 300 
            filter_dim: 700 # 600 
            layer_num: 6
            head_num: 8
          decoder:
            src_dim: 300
            hidden_dim: 200 # 128
            filter_dim: 600 # 512
            layer_num: 3
            head_num: 8
        ```
        - 메모리가 32400MiB / 32480MiB
    - beam search?
    - teacher forcing schedule?
- 임기홍
  - 데이터(AIDA + ORIGINAL) 학습하는데 LABEL IMBALANCE 문제가 발생할것같음 
  - AIDA 데이터셋이 상대적으로 LABEL이 적게 있음

### 해볼 거
- Beam search
    - 단순 그리디로 순차적으로 가장 높은 확률의 값으로 예측해나가는 것이 아니라, k개의 후보를 동시에 고려해서 (여러가지 조건부 확률을 계산해) 최종적으로 문장이 나왔을 때의 확률이 (고려된 것들 중에서) 가장 높게 예측하는 방법.
    - 일부가 잘못된 수식이 고쳐지거나 (후처리 효과), 
    - 어디다 넣을 수 있을가
        - STARN.py - Transformerdecoder의 forward의 torch.argmax부분을 변경?
        - transformer에서의 beamsearch를 따로 확인해야 할듯
    - 참고할 만한 github
        - [Pytorch-seq2seq-Beam-Search](https://github.com/312shan/Pytorch-seq2seq-Beam-Search/blob/master/model.py)
        - transformer-pytorch: [beam.py](https://github.com/dreamgonfly/transformer-pytorch)
        - [자연어 생성에서의 Beam Search](https://littlefoxdiary.tistory.com/4#:~:text=Beam%20Search%EB%8A%94%20%ED%83%90%EC%9A%95%20%EB%B0%A9%EB%B2%95,%EB%8B%A4%EC%9D%8C%20%EB%8B%A8%EA%B3%84%EB%A5%BC%20%ED%83%90%EC%83%89%ED%95%9C%EB%8B%A4.)
        - [beam-search-decoder-natural-language-processing](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/) 

### 코드공유

#### Adaptive2DPositionEncoder

```python

class Adaptive2DPositionEncoder(nn.Module):
    def __init__(self, in_channels, max_h=64, max_w=128, dropout=0.1):
        super(Adaptive2DPositionEncoder, self).__init__()

        h_position_encoder = self.generate_encoder(in_channels, max_h)
        self.h_position_encoder = h_position_encoder.transpose(0, 1).view(1, in_channels, max_h, 1)

        w_position_encoder = self.generate_encoder(in_channels, max_w)
        self.w_position_encoder = w_position_encoder.transpose(0, 1).view(1, in_channels, 1, max_w)

        self.h_scale = self.scale_factor_generate(in_channels)
        self.w_scale = self.scale_factor_generate(in_channels)
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.dropout = nn.Dropout(p=dropout)

    def generate_encoder(self, in_channels, max_len):
        pos = torch.arange(max_len).float().unsqueeze(1)
        i = torch.arange(in_channels).float().unsqueeze(0)
        angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / in_channels)
        position_encoder = pos * angle_rates
        position_encoder[:, 0::2] = torch.sin(position_encoder[:, 0::2])
        position_encoder[:, 1::2] = torch.cos(position_encoder[:, 1::2])
        return position_encoder  # (Max_len, In_channel)

    # 큰 차이점, 이게 존재
    def scale_factor_generate(self, in_channels): # alpha인듯
        scale_factor = nn.Sequential( 
            nn.Conv2d(in_channels, in_channels, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels, in_channels, kernel_size=1),
            nn.Sigmoid()
        )
        return scale_factor

    def forward(self, x):
        b, c, h, w = x.size()

        avg_pool = self.pool(x)

        # 학습가능한 파라미터?
        h_pos_encoding = self.h_scale(avg_pool) * self.h_position_encoder[:, :, :h, :].to(x.get_device())
        w_pos_encoding = self.w_scale(avg_pool) * self.w_position_encoder[:, :, :, :w].to(x.get_device())

        out = x + h_pos_encoding + w_pos_encoding

        out = self.dropout(out)
        return out
        
class TransformerEncoderFor2DFeatures(nn.Module):
    ...
    # self.positional_encoding = PositionalEncoding2D(hidden_dim) # 원본 부분
    self.positional_encoding = Adaptive2DPositionEncoder(hidden_dim)
    ...
```