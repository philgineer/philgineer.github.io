---
title: "Boostcamp AI Tech (P1 - Day04)"
date: 2021-04-01
layout: post
tags: [Naver Boostcamp, daily report]
use_math: true
---

## 개인 Project 진행 상황
* ### transform 적용
    * transform을 만들어놓고, dataset의 getitem에서 적용하고 있지 않았음!
    * 그래서 image size가 resize되지 않아 reshape이 추가적으로 필요했음
    * submission inference 시 PIL Image 파일이 아니라는 오류가 뜨는데, 원래는 training 할 때도 떴어야하는 오류였음. (내가 만든 train dataset 클래스에서는 Image.open 대신 cv2.imread를 사용했음)
* ### 1차 학습 결과
    * 오류 수정 + EfficientNet b0 사용 후 (transform 적용 전)
    * Best validation Acc: 0.79
    * Test Acc: 0.42
    * Test f1 score: 0.46
    * Hyper param
        * adam
        * lr: 3e-4
        * epoch: 2
        * batchsize: 20
        * loss: CE
* ### 2차 학습 결과
    * 같은 모델 + transform 적용 후 결과
    * Best validation Acc: 0.61
    * Test Acc: 0.73
    * **Test f1 score: 0.66** (당일 기준 50위권!)
    * Hyper param
        * optim, loss 동일
        * lr: 1e-3
        * lr scheduler
            * stepsize: 3
            * gamma: 0.2
        * batchsize: 128
        * transform
            * grayscale
            * random rotation(10)
            * centercrop(450, 360)
            * horizontal flip
            * resize(200, 200)
            * normalize
* ### train / validation split 사람 단위로 구현
    ```python
    # train val split (1명당 이미지 7장씩 존재)
    indice = [i for i in range(18900 // 7)]
    random.seed(42)
    validation_mask = random.sample(indice, int(len(indice) * 0.2))

    class MyDataset(Dataset):
        def __init__(...):

            if is_Train == True:
                for i in range(len(img_path)):
                    if i // 7 not in validation_mask:
                        img_path_sep.append(img_path[i])
            else:
                for i in range(len(img_path)):
                    if i // 7 in validation_mask:
                        img_path_sep.append(img_path[i])
    ```
<br><br>

## 생각해볼 부분
* oversampling
    * weighted ramdom sampler
* class, loss 가중치
    * class-weights
    * focal loss
* 노인 외부 데이터 수집, mask 생성 후 feed
* efficientnet b0 외에 더 데이터 표현에 적합한 모델 search
* cutmix?
<br><br>

## Peer session
* (한마루님) efficientnet b0, pretrained, 5.3M prarams, no augmentation, 1 epoch 당 훈련 시간 3분 30초!
* ### backbone model freeze하는 법
    * 불러올 때 param
    * layer 별로 requires_grad 값 조정
* ### class-weights & focal loss
    * class weights
        * 클래스 개수에 비례해서, mini batch의 loss 계산 시 개수 적은 클래스의 loss는 크게, 개수 많은 클래스의 loss는 적게 weighted sum.
        * my link share: [TF 불균형 데이터 분류 - 클래스 가중치](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#%ED%81%B4%EB%9E%98%EC%8A%A4_%EA%B0%80%EC%A4%91%EC%B9%98)
    * focal loss
        * (태진님) loss 계산 시, cross entropy에 $(1 - p_t)^\gamma$ 텀을 곱해서 ($\gamma = 1$일 때 cross entropy와 동일), 로짓값이 높게 나온 쉽게 예측되는 easy examples에 대한 loss 작게 만듦. 즉 잘못 분류된 hard examples에 대한 상대적 loss 가중치를 높이는 역할.
        * [자료1](https://ropiens.tistory.com/83) [자료2](https://www.slideshare.net/ssuser06e0c5/focal-loss-detection-classification)
* my link share: [Best loss function for F1-score metric](https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric)
<br><br>

## 강의
* ### Model
    * Loss
        * loss 함수 = cost 함수 = error 함수
        * nn.Module family (forward 정의되어 있음)
        * loss.backward()
            * requires_grad=False 제외한 파라미터의 grad 값 업데이트
        * focal loss
            * class imbalance 문제가 있을 때, 맞춘 확률에 높은 class는 조금만 loss 반영, 맞춘 확률이 낮은 class는 loss 크게 부여
        * label smoothing loss
            * label을 onehot 표현이 아닌 soft하게 표현해 일반화 성능 높임 (0, 1 두가지가 아닌 0.025, 0.9 등으로)
    * Optimizer
        * LR scheduler
            * StepLR
            * CosineAnnealingLR
            * ReduceLROnPlateau
                * 더 이상 성능 향상이 없을 때 LR 감소
    * Metric
        * 학습에 직접적으로 사용되진 않지만, 모델을 객관적으로 평가할 지표이기 때문에 잘 설정할 필요가 있음
* ### Training process
    * model.train()
    * for batch
        * zero_grad()
        * forward
        * loss 계산
        * backward
        * optimizer.step(): grad update
* ### 응용: Gradient accumulation
    * model.train()
    * optimizer.zero_grad()
    * for batch
        * forward
        * loss / NUM_ACCUM
        * backward
        * if iter % NUM_ACCUM == 0:
            * optimizer.step()
            * optimizer.zero_grad()
* ### Inference process
    * model.eval()
    * with torch.no_grad():
        * forward
        * metric 계산
* Pytorch lightning
    * keras처럼 간편하게 사용 가능
<br><br>